{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "\\begin{aligned}  \n",
    "\\textbf{ANLP:Assignment 7}  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$  \n",
    "\\begin{aligned}\n",
    "========================\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$  \n",
    "\\begin{aligned} \n",
    "\\textbf{Archana Molasi,Dwipam Kataria,Krishna Mahajan,Sanjana Agrawal}   \n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Theory**  \n",
    "\n",
    "*Levels of Sentiment Analysis*:  \n",
    "A sentiment is a complex, multi dimensional entity.  \n",
    "Document level: The entire document is considered a single entity and a single sentiment of the document is derived.  \n",
    "Sentence level: Each sentence is analyzed for positive, negative or neutral sentiment.  \n",
    "Aspect and Entity level: In this type of analysis, we directly look at the sentiment instead of sentence structures or other language constructs.   \n",
    "\n",
    "*Sentiment lexicon and issues*:\n",
    "\n",
    "Some words are easily classified as positive or negative sentiment words.  \n",
    "eg: good, wonderful, great are positive sentiment words  \n",
    "terrible, bad, miserable are negative sentiment words.  \n",
    "Apart from these words, there are phrases which are used to express sentiments.\n",
    "Some of the issues associated with sentiment analysis are:  \n",
    "Sarcastic comments:  It is difficult to analyse the sentiment of a sarcastic sentence.  \n",
    "Eg: What a great day? I lost my job.  \n",
    " The sentiment word sucks has an opposite sentiment in both the sentences.  \n",
    "Sentiment with no sentiment words: Many sentences without sentiment words can also express an opinion.  \n",
    "Eg: This washing machine uses a lot of water.   \n",
    "\n",
    "\n",
    "Although the sentence has a positive sentiment word, it expresses a negative sentiment.  \n",
    "Neutral sentences: Some sentences are just a form of question or suggestions which use a positive sentiment word.\n",
    "Eg: Does anyone know a delicious, easy breakfast to make?  \n",
    "This is just a question and intends to express no sentiment, although it uses positive sentiment word.  \n",
    "Opposite orientation of sentiment word: Another common issue is when a sentiment word expresses an opposite sentiment.  \n",
    "Eg: Life sucks vs this vaccum cleaner sucks all the dust.  \n",
    "\n",
    "\n",
    "** Sentiment Analysis Project Overview **  \n",
    "- As part of our research on sentiment analysis we worked on [Kaggle competition on Movie sentiment sentiment analysis](https://www.kaggle.com/c/word2vec-nlp-tutorial). \n",
    "\n",
    "**Data Overview**  \n",
    "- sentiment analysis project \n",
    "- 100,000 movie reviews   \n",
    "- 25,000  Train reviews & 25,000 Test reviews   \n",
    "- Another 50,000 unlabeled reviews  \n",
    "- Evaluation metric was AUC ROC  \n",
    "\n",
    "**Data Exploration**  \n",
    "- we started with Data Exploration and basic Data Understaning. \n",
    "- The dataset had three columns id,sentiment,review   \n",
    "- The Movie reviews were like typical paragraphs in English with also punctuations,numbers and emojis  \n",
    "- The good thing about the Dataset was both the classes were balanced i,e 50% positive sentiment and 50% negative sentiments.So any good classifier we build should have accuracy lot more than 50%   \n",
    "- Also during data exploration we observed that reviews had HTML tags as typical in any online data.   \n",
    "\n",
    "![](./images/picture1.png)  \n",
    "\n",
    "![](./images/picture2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning** \n",
    "- After Data Exploration and Data Understanding we started with Data Cleaning\n",
    "- We used python's library BeautifulSoup to remove HTML tags  \n",
    "- We then removed numbers,punctuation ,emojis using regular expression  \n",
    "- We did typical text cleaning steps like lowercasing all the words,Removing Stopwords  \n",
    "- And finally using NLTK library  \n",
    "    - Tokenized all the words in the review  \n",
    "    - And lemmatized all the tokens  \n",
    "- Finally joined all the Tokens to retrieve the cleaned reviews  \n",
    "- So we wrote python function of this steps for all the 100K reviews and store all the reviews in list of list format\n",
    "  for training,test & unlabeled reviews   \n",
    "  \n",
    "  ![](./images/picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Modeling**  \n",
    "- Now after all the Exploration and Data Cleaning ,for main part of sentiment Analysis i,e Data Modeling we used three different approaches and  2 different classifiers such logistic,random forest  \n",
    "\n",
    "**First Approach**  \n",
    "- First approach was Bag of words , one gram model  \n",
    "- So in this approach our feature space was 5000 most frequent words  \n",
    "- So now each review was 5000 feature long with each entry equal to number of occurence of that word   \n",
    "- we used scikit learn CountVectorizer function for this but we have also coded it ourselves  \n",
    "- Now when we run our classifier on this approach we got following results \n",
    "    - Logistic Regression 0.55  \n",
    "    - Random Forest       0.59  \n",
    "- Also we did parameter tuning to some extent for each of this algorithms\n",
    "\n",
    "**Sample Code for Bag of words models**  \n",
    "![](./images/picture4.png)   \n",
    "  \n",
    "\n",
    "\n",
    "**Kaggle Result Logistic Regression** \n",
    "![](./images/picture5.png)   \n",
    "\n",
    "\n",
    "**Kaggle Result Random Regression**  \n",
    "![](./images/picture6.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Approach**  \n",
    "- Now in the Second Approach we used Term Frequency Inverse Document Frequency one gram model  \n",
    "- Again we used maximum of 5000 features  \n",
    "- we used Scikit learn Tf-IDf vectorizer module for this  \n",
    "- Now when i run my classifier i got the following scores \n",
    "  - Logistic Regression 0.87  \n",
    "  - Random FOrest       0.77  \n",
    "\n",
    "**Sample Code for Tf-IDF models** \n",
    "![](./images/picture7.png)    \n",
    "\n",
    "\n",
    "\n",
    "**Kaggle Result Logistic Regression** \n",
    "![](./images/picture8.png)   \n",
    "\n",
    "\n",
    "**Kaggle Result Random Regression**  \n",
    "![](./images/picture9.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Approaches**  \n",
    "\n",
    "- Term Frequency – Inverse document frequency(TF-IDF) ,Two gram model  \n",
    "- Vector Averaging (Unsupervised learning using Google’s word2Vec algorithm)\n",
    "- Bag of Centroids (Unsupervised Learning using Google’s word2Vec algorithm and K-means clustering ) \n",
    "- Ensemble of Naïve Bayes and Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Work Division**  (no of hours)\n",
    "\n",
    "| Work Division | Theory | Data Exploration | Data Cleaning | Data Modeling |\n",
    "|---------------|--------|------------------|---------------|---------------|\n",
    "| Archana       | 3      | 1                | 1             | 1             |\n",
    "| Dwipam        | 1      | 3                | 1             | 1             |\n",
    "| Krishna       | 1      | 1                | 1             | 2             |\n",
    "| Sanjana       | 1      | 1                | 3             | 1             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Refrences**  \n",
    "- Kaggle Discussion \n",
    "- http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf\n",
    "- Sentiment-Analysis-tutorial-AAAI-2011.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
