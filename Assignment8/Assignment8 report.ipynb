{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "\\begin{aligned}  \n",
    "\\textbf{ANLP:Assignment 8}    \n",
    "\\end{aligned} \n",
    "\\begin{aligned} \n",
    "\\textbf{Sentiment Analysis of Movie Reiews}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$  \n",
    "\\begin{aligned}\n",
    "========================\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$  \n",
    "\\begin{aligned} \n",
    "\\textbf{Dwipam Katariya,Krishna Mahajan,Sanjana Agrawal}   \n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Motivation](#Motivation)  \n",
    "- [Theory](#Theory)\n",
    "- [Sentiment_Analysis_Project_Overview](#Sentiment_Analysis_Project_Overview)  \n",
    "    -  Data Overview \n",
    "    -  File Descriptions  \n",
    "    -  Data Fields  \n",
    "    -  Exploratory Analysis \n",
    "    -  Data Cleaning \n",
    "    -  Data Modeling  \n",
    "        - Approach 1 : Bag of words,one gram model\n",
    "        - Approach 2 : Term Frequency Inverse Document Frequency one gram model \n",
    "        - Approach 3:  Term Frequency – Inverse document frequency(TF-IDF) ,Two gram model\n",
    "        - Approach 4:  Vector Averaging (Unsupervised learning using Google’s word2Vec algorithm)\n",
    "        - Appraoch 5:  Bag of Centroids (Unsupervised Learning using Google’s word2Vec algorithm and                                          K-means clustering ) \n",
    "        - Approach 6 : Ensemble of Naïve Bayes and Logistic Regression\n",
    "- [Conclusion](#Conclustion) \n",
    "- [Future Scope](#Future Scope) \n",
    "- [Work Division](#Work Division) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Motivation**    \n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers\n",
    "Sentiment analysis is the task of identifying whether the opinion expressed in a  text is positive or negative in general,or about a given topic. For example: **I am so happy today, good morning to everyone”, is a general positive text, and the text:** \n",
    "**“Django  is  such a good movie, highly recommends 10/10”**,express positive\n",
    "sentiment toward the movie, named Django, which is considered as the topic of \n",
    "this text. Sometimes, the task of identifying the exact sentiment is not so clear \n",
    "even for humans, for example in  the text:! \n",
    "**“I'm surprised so many people put Django in their favorite films ever list, I felt it was a good watch but definitely not that good”,** \n",
    "the sentiment expressed by the author toward the movie is probably \n",
    "positive, but surely not as good as in the message that was mentioned above. In \n",
    "many other cases, identifying the sentiment of a given text is very difficult for an \n",
    "algorithm,even when it looks easy from  a human perspective, for  example:**“if\n",
    "you haven't seen Django,you're not worth my time.If you plan to see it, there's\n",
    "hope for you yet.”**  \n",
    "Well-known tasks for Natural Language Processing (NLP) involve general sentiment analysis for\n",
    "various types of texts or speech. Such models are capable to output the general sentiment of a\n",
    "sentence or piece of text. However, a special subset of tasks such as mining product reviews requires\n",
    "more than high level sentiment analysis, rather it requires entity level sentiment analysis (or aspect\n",
    "specific sentiment analysis) on the level of review-specific features (for instance the sentiment/score\n",
    "of ”display” in a product review of a smart phone). Other applications are public opinion predictions(Movie reviews),\n",
    "opinion mining or emotion detection. In many applications, the goal is to perform this sentiment\n",
    "analysis over time. This analysis can naturally be used for other tasks such as recommender systems\n",
    "or summarizing tasks.  \n",
    "\n",
    "\n",
    "# **Theory**  \n",
    "\n",
    "**Levels of Sentiment Analysis**:  \n",
    "A sentiment is a complex, multi dimensional entity.  \n",
    "Document level: The entire document is considered a single entity and a single sentiment of the document is derived.  \n",
    "Sentence level: Each sentence is analyzed for positive, negative or neutral sentiment.  \n",
    "Aspect and Entity level: In this type of analysis, we directly look at the sentiment instead of sentence structures or other language constructs.   \n",
    "\n",
    "**Sentiment lexicon and issues**:\n",
    "\n",
    "Some words are easily classified as positive or negative sentiment words.  \n",
    "eg: good, wonderful, great are positive sentiment words  \n",
    "terrible, bad, miserable are negative sentiment words.  \n",
    "Apart from these words, there are phrases which are used to express sentiments.\n",
    "Some of the issues associated with sentiment analysis are:  \n",
    "Sarcastic comments:  It is difficult to analyse the sentiment of a sarcastic sentence.  \n",
    "Eg: What a great day? I lost my job.  \n",
    " The sentiment word sucks has an opposite sentiment in both the sentences.  \n",
    "Sentiment with no sentiment words: Many sentences without sentiment words can also express an opinion.  \n",
    "Eg: This washing machine uses a lot of water.   \n",
    "\n",
    "\n",
    "Although the sentence has a positive sentiment word, it expresses a negative sentiment.  \n",
    "Neutral sentences: Some sentences are just a form of question or suggestions which use a positive sentiment word.\n",
    "Eg: Does anyone know a delicious, easy breakfast to make?  \n",
    "This is just a question and intends to express no sentiment, although it uses positive sentiment word.  \n",
    "Opposite orientation of sentiment word: Another common issue is when a sentiment word expresses an opposite sentiment.  \n",
    "Eg: Life sucks vs this vaccum cleaner sucks all the dust.  \n",
    "\n",
    "\n",
    "# ** Sentiment_Analysis_Project_Overview **  \n",
    "- As part of our research on sentiment analysis we worked on [Kaggle competition on Movie sentiment sentiment analysis](https://www.kaggle.com/c/word2vec-nlp-tutorial). \n",
    "\n",
    "**Data Overview**  \n",
    "- sentiment analysis project \n",
    "- 100,000 movie reviews   \n",
    "- 25,000  Train reviews & 25,000 Test reviews   \n",
    "- Another 50,000 unlabeled reviews  \n",
    "- Evaluation metric was AUC ROC  \n",
    "\n",
    "\n",
    "\n",
    "** File descriptions ** \n",
    "\n",
    "- **labeledTrainData** :The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "- **testData** :The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one  \n",
    "\n",
    "- **unlabeledTrainData** :An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review.  \n",
    "\n",
    "- **sampleSubmission** :  A comma-delimited sample submission file in the correct format. \n",
    "\n",
    "\n",
    "** Data Fields ** \n",
    "\n",
    "- **id** : Unique ID of each review  \n",
    "- **sentiment** : Sentiment of the review; 1 for positive reviews and 0 for negative reviews  \n",
    "- **review** : Text of the review\n",
    "\n",
    "\n",
    "**Data Exploration**  \n",
    "- we started with Data Exploration and basic Data Understaning. \n",
    "- The dataset had three columns id,sentiment,review   \n",
    "- The Movie reviews were like typical paragraphs in English with also punctuations,numbers and emojis  \n",
    "- The good thing about the Dataset was both the classes were balanced i,e 50% positive sentiment and 50% negative sentiments.So any good classifier we build should have accuracy lot more than 50%   \n",
    "- Also during data exploration we observed that reviews had HTML tags as typical in any online data.   \n",
    "\n",
    "![](./images/picture1.png)  \n",
    "\n",
    "![](./images/picture2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning** \n",
    "- After Data Exploration and Data Understanding we started with Data Cleaning\n",
    "- We used python's library BeautifulSoup to remove HTML tags  \n",
    "- We then removed numbers,punctuation ,emojis using regular expression  \n",
    "- We did typical text cleaning steps like lowercasing all the words,Removing Stopwords  \n",
    "- And finally using NLTK library  \n",
    "    - Tokenized all the words in the review  \n",
    "    - And lemmatized all the tokens  \n",
    "- Finally joined all the Tokens to retrieve the cleaned reviews  \n",
    "- So we wrote python function of this steps for all the 100K reviews and store all the reviews in list of list format\n",
    "  for training,test & unlabeled reviews    \n",
    "- Dealing with Punctuation,Numbers and Stopwords: NLTK and regular expressions\n",
    "  - For many probelms,it makes sense to remove punctuation. On the other hand, in this case,we are tackling a sentiment analysis problem, and it is possible that \"!!!\" or \":-(\" could carry sentiment, and should be treated as words.  \n",
    "    - Similarly we'll remove numbers,but there are other ways of dealing with them. For example,we could treat them as words,or replace them all with a placeholder string such as \"NUM\".   \n",
    "    - To remove puncutation and numbers,we will use a package for dealing with regular expressions,called **re**\n",
    "  \n",
    "  ![](./images/picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Modeling**  \n",
    "- Now after all the Exploration and Data Cleaning ,for main part of sentiment Analysis i,e Data Modeling we used Five different approaches and  2 different classifiers such logistic,random forest  \n",
    "\n",
    "# 1. **First Approach :Bag of words,one gram model**  \n",
    "- First approach was Bag of words , one gram model  \n",
    "- So in this approach our feature space was 5000 most frequent words  \n",
    "- So now each review was 5000 feature long with each entry equal to number of occurence of that word   \n",
    "- we used scikit learn CountVectorizer function for this but we have also coded it ourselves  \n",
    "- Now when we run our classifier on this approach we got following results \n",
    "    - Logistic Regression 0.55  \n",
    "    - Random Forest       0.59  \n",
    "- Also we did parameter tuning to some extent for each of this algorithms\n",
    "\n",
    "**Sample Code for Bag of words models**  \n",
    "![](./images/picture4.png)   \n",
    "  \n",
    "\n",
    "\n",
    "**Kaggle Result Logistic Regression** \n",
    "![](./images/picture5.png)   \n",
    "\n",
    "\n",
    "**Kaggle Result Random Regression**  \n",
    "![](./images/picture6.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Second Approach: Term Frequency Inverse Document Frequency one gram model**  \n",
    "- Now in the Second Approach we used Term Frequency Inverse Document Frequency one gram model  \n",
    "- Again we used maximum of 5000 features  \n",
    "- we used Scikit learn Tf-IDf vectorizer module for this  \n",
    "- Now when i run my classifier i got the following scores \n",
    "  - Logistic Regression 0.87  \n",
    "  - Random FOrest       0.77  \n",
    "\n",
    "**Sample Code for Tf-IDF models** \n",
    "![](./images/picture7.png)    \n",
    "\n",
    "\n",
    "\n",
    "**Kaggle Result Logistic Regression** \n",
    "![](./images/picture8.png)   \n",
    "\n",
    "\n",
    "**Kaggle Result Random Regression**  \n",
    "![](./images/picture9.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Third Approach:Term Frequency – Inverse document frequency(TF-IDF) ,Two gram model**   \n",
    "- Now in the third approach was Tf-IDf but now with Two Gram Model \n",
    "- So we increased the max_features = 12000 (6000 for one gram + 6000 for two gram \n",
    "- Now when we run my classifier we got the following scores\n",
    "  - Logistic Regression 0.86\n",
    "  - Random FOrest 0.81  \n",
    "- So result of the Random forest got little better. But overall we didnt find any improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using HashingVectorizer to control Memory Usage.\n",
    "corpustr = clean_train_reviews\n",
    "estimators = [(\"tfidf\", TfidfVectorizer(stop_words='english',\n",
    "             ngram_range = ( 1 , 1 ),analyzer=\"word\",\n",
    "             max_df = .57 , binary=False ,max_features =6000, \n",
    "             token_pattern=r'\\w+' , sublinear_tf=False) ),\n",
    "             (\"hash\", HashingVectorizer ( stop_words='english',\n",
    "              ngram_range = ( 1 , 2 ),n_features  =6000,\n",
    "            analyzer=\"word\",token_pattern=r'\\w+', binary =False))] \n",
    "\n",
    "tfidftr = FeatureUnion(estimators).fit_transform(corpustr).todense()\n",
    "corpusts = clean_test_reviews\n",
    "tfidfts = FeatureUnion(estimators).transform(corpusts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Result Logistic Regression** \n",
    "![](./images/picture8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ** Fourth Approach : Vector Averaging (Unsupervised learning using Google’s word2Vec algorithm)**   \n",
    "- Now After the first three supervised approaches we moved to unsupervised alogrithm like Google Word2Vec   \n",
    "- So till now all the approaches we used were basically counting the word frequency in some manner and then feeding to ML algorithms   \n",
    "- None of the algorithms above were actually focusing on context of the sentences rather  \n",
    "- Also till now we haven't used another set of 50,000 unlabeled review  \n",
    "- Word2Vec is unsupervised algorithm which captures semantic relations in the sentences   \n",
    "- The algorithm represent each word vector such that the vector of synoyms tend to be very similar in terms of cosine similarity  \n",
    "- we used python gensim library package to use word2Vec algorithm  \n",
    "- So in this approach we converted the 75,000 reviews into almost 0.8 million sentences. we stored this sentences in list of list format   \n",
    "- Then we feed this sentences to the algorithms with some other parameters  (such as context size,num_features,min number of features)  \n",
    "- Now after the algorithm finished it created a vocabulary of 16000 + words and each word is represented as a vector in 300 dimensional space    \n",
    "- Now to represent each review as a vector we added all the vecors of the individual word to get 300 dimensional vector for each review and then divided with total no of words in that vector and hence vector Averaging  \n",
    "- Now when we run our two classifier we got the following scores \n",
    "  - Logistic Regression 0.86 \n",
    "  - Random Forest       0.83  \n",
    "- Again the performance of Random forest was increased but overall this approach didnt performed any better than tf-idf one gram model  \n",
    "- As said in its document the algorithm performs exceptionally well if fed with billion of sentences, here i gave just 0.8 million sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Similarity Results** \n",
    "![](./images/picture11.png)  \n",
    "\n",
    "![](./images/picture12.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Results Approach 5 Logistic**  \n",
    "![](./images/picture13.png)  \n",
    "\n",
    "\n",
    "**Kaggle Results Approach 6  Random Forest**\n",
    "![](./images/picture14.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ** Fifth Approach : Bag of Centroids (Unsupervised Learning using Google’s word2Vec algorithm and K-means clustering ) **   \n",
    "- Word2Vec creates clusters of semantically related words,so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"Vector Quantization\". To accomplish this,we first need to find the centers of the word clusters,which we can do by using a clustering algorithm such as K-means\n",
    "- In K_Means , the one parameter we need to set is \"K\",or the number of clusters.How should we decide how many clusters to create? Trial and error suggested that small clusters,with an average of only 5 words or so per cluster ,gave better results than large clusters with many words  \n",
    "- In this approach we followed little more sophisticated thinking following word2vec model.  \n",
    "- Now we clustered each word in the original 16000+ vocabulary in one of the 3298 clusters  \n",
    "- Now each review we represented as a feature of length 3298 with each entry of number of words in that cluster  \n",
    "- Now when we  run my classifier i got the following scores \n",
    "  - Logistic Regression 0.85 \n",
    "  - Random FOrest       0.85  \n",
    "- This approach performed better than previous word2vec approach but clearly not best \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "import time \n",
    "\n",
    "start = time.time() \n",
    "# Set \"K\" (num_clusters) to be 1/5th of the Vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0 \n",
    "num_clusters = int(word_vectors.shape[0]/5)\n",
    "print(num_clusters)\n",
    "# Initialize a K-means object and use it to extract centroids \n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters) \n",
    "idx = kmeans_clustering.fit_predict(word_vectors)  \n",
    "\n",
    "#Get the end time and print how long the process took \n",
    "end = time.time()\n",
    "elapsed = end - start  \n",
    "print(\"Time taken for K Means clustering: \",elapsed,\"seconds\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clustering Results Kmeans**  \n",
    "- ![](./images/picture15.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  ** Sixth Approach : Ensemble of Naïve Bayes and Logistic Regression**\n",
    "- Now at last inspired by one of the Kaggle discussion we just tried ensemble of lot of ML algorithms  \n",
    "- Using Tf-Idf we created a feature set of 20,000 with 3 gram model\n",
    "- In one such approach we tried a ensemble of Naive Bayes and regularized logistic\n",
    "  with some parameter tuning \n",
    "- Suprisingly this approach gave us a score of 0.96 (within top 10% in Kaggle Rank)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "model1 = MultinomialNB(alpha=0.0005)\n",
    "model1.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "model2 = LogisticRegression(penalty=\"l1\")\n",
    "model2.fit( train_data_features, train[\"sentiment\"] )\n",
    "\n",
    "p1 = model1.predict_proba( test_data_features )[:,1]\n",
    "p2 = model2.predict_proba( test_data_features )[:,1]\n",
    "\n",
    "print(\"Writing results...\")\n",
    "\n",
    "output = pd.DataFrame( data = { \"id\": test[\"id\"], \"sentiment\": (p1 + p2)/2 } )\n",
    "output.to_csv( 'output_file.csv', index = False, quoting = 3 )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Result**  \n",
    "- ![](./images/picture16.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "- In this mini project we tried six different approaches to model sentiment analysis \n",
    "- We used both supervised and usupervised approaches  \n",
    "- We could observe that if we make our model complexity from bi-gram to tri-gram, there's significant improvement in performance \n",
    "- In unsupervised approaches such as Word2Vec,we could use context of the sentences too rather than just frequency of them  \n",
    "- Atlast we got maximum accuracy when we used ensemble of Naieve Bayes and Logistic Regression, which validates the fact that ensembling in machine learning always outperforms single machine learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Work Division**  (no of hours)\n",
    "\n",
    "| Work Division | Theory | Data Exploration | Data Cleaning | Data Modeling |\n",
    "|---------------|--------|------------------|---------------|---------------|\n",
    "| Dwipam        | 1      | 3                | 1             | 1             |\n",
    "| Krishna       | 1      | 1                | 1             | 2             |\n",
    "| Sanjana       | 1      | 1                | 3             | 1             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Refrences**  \n",
    "- Kaggle Discussion \n",
    "- http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf\n",
    "- Sentiment-Analysis-tutorial-AAAI-2011.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
