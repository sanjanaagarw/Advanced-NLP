{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels   \n",
    "\n",
    "# File descriptions  \n",
    "\n",
    "- **labeledTrainData** :The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "- **testData** :The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one  \n",
    "\n",
    "- **unlabeledTrainData** :An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review.  \n",
    "\n",
    "- **sampleSubmission** :  A comma-delimited sample submission file in the correct format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fields \n",
    "- **id** : Unique ID of each review  \n",
    "- **sentiment** : Sentiment of the review; 1 for positive reviews and 0 for negative reviews  \n",
    "- **review** : Text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric \n",
    "- Area under the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports  \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup \n",
    "import re  \n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn import grid_search \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import FeatureUnion  \n",
    "from sklearn.cross_validation import KFold, cross_val_score  \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.cross_validation import StratifiedKFold    \n",
    "from sklearn.svm import LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Test Datasets \n",
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"header=0\" indicates that the first line of the contains column names.**\n",
    "- **\"delimiter = \\t\" indicates that the fields are separated by tabs** \n",
    "- **\"Quoting=3\" tells python to ignore doubled quotes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shape of training Data\n",
    "train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking look of training Data\n",
    "train.columns.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking Balance of Training Data\n",
    "train[\"sentiment\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample review \n",
    "train[\"review\"][0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **As can be seen there are HTML tags ,abbreviations,puncutation-all common issues when processing text from online.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "#Removing HTML Markup using BeautifulSoup4 python package.  \n",
    "\n",
    "#Initializing BeautifulSoup object on a single movie review \n",
    "example1 = BeautifulSoup(train[\"review\"][0],\"lxml\")  \n",
    "\n",
    "#Printing the raw review and then the output of get_text(),for comparision\n",
    "print(example1.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calling get_text() gives us the text of the review,without tags or markup.  \n",
    "- It is not considered a reliabel practice to remove markup using regular expression,so even for an application as simple as this,it's usually best to use a package like **BeautifulSoup**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Punctuation,Numbers and Stopwords: NLTK and regular expressions\n",
    "- For many probelms,it makes sense to remove punctuation. On the other hand, in this case,we are tackling a sentiment analysis problem, and it is possible that \"!!!\" or \":-(\" could carry sentiment, and should be treated as words.  \n",
    "- Similarly we'll remove numbers,but there are other ways of dealing with them. For example,we could treat them as words,or replace them all with a placeholder string such as \"NUM\".   \n",
    "- To remove puncutation and numbers,we will use a package for dealing with regular expressions,called **re** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "#using regular expressions to do a find-and-replace \n",
    "letters_only = re.sub(\"[^a-zA-Z]\",\" \",example1.get_text())\n",
    "print(letters_only)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **^ means not** \n",
    "- **[] indicates group**   \n",
    "- **Find anything that is not a lowercase letter (a-z) or an upper case letter (A-Z), and replace it with a space** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"
     ]
    }
   ],
   "source": [
    "#converting reviews to lower case and split them into individual words(tokenization)\n",
    "letters_only=letters_only.lower() \n",
    "words = nltk.tokenize.word_tokenize(letters_only)\n",
    "words = [w for w in words if not w in open(\"./data/stopwords.txt\")] \n",
    "print(words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Train & Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Making a set of all stopwords and lemmatizer object\n",
    "stopwords= set(w.rstrip() for w in open('./data/stopwords.txt'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()  \n",
    "\n",
    "\n",
    "\n",
    "#Putting it all together \n",
    "def my_tokenizer(s): \n",
    "    s = BeautifulSoup(s,\"lxml\") \n",
    "    s = s.get_text() \n",
    "    s = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
    "    s = s.lower() \n",
    "    tokens=nltk.tokenize.word_tokenize(s)\n",
    "    tokens =[wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    tokens = [token for token in tokens if token not in stopwords] \n",
    "    return \" \".join(tokens) \n",
    "\n",
    "#Total reviews \n",
    "num_reviews = train[\"review\"].size \n",
    "\n",
    "#Initialize an empty list to hold the clean reviews \n",
    "clean_train_reviews = [] \n",
    "\n",
    "#Loop over each review;create an index i that goes from 0 to the lenght\n",
    "#of the movie review list  \n",
    "for i in range(0,num_reviews): \n",
    "    if((i+1)%1000 ==0):\n",
    "        print(\"Review %d of %d\\n\" %(i+1,num_reviews))\n",
    "    clean_train_reviews.append(my_tokenizer(train[\"review\"][i])) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In python searching a set is much faster than searching a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_train_reviews))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"./data/testData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "print(test.shape) \n",
    "\n",
    "#Putting it all together \n",
    "def my_tokenizer(s): \n",
    "    s = BeautifulSoup(s,\"lxml\")\n",
    "    s = s.get_text() \n",
    "    s = re.sub(\"[^a-zA-Z]\",\" \",s)\n",
    "    s = s.lower() \n",
    "    tokens=nltk.tokenize.word_tokenize(s)\n",
    "    tokens =[wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "#Total reviews \n",
    "num_reviews = test[\"review\"].size \n",
    "\n",
    "#Initialize an empty list to hold the clean reviews \n",
    "clean_test_reviews = [] \n",
    "\n",
    "#Loop over each review;create an index i that goes from 0 to the lenght\n",
    "#of the movie review list  \n",
    "for i in range(0,num_reviews): \n",
    "    if((i+1)%1000 ==0): \n",
    "        print(\"Review %d of %d\\n\" %(i+1,num_reviews))\n",
    "    clean_test_reviews.append(my_tokenizer(test[\"review\"][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach1 : Creating Features from Bag of words (CountVectorizer,OneGram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training reviews cleaned up,how do we convert them to some kind of numeric representation for machine learning. \n",
    "One common approach is called **Bag of Words**   \n",
    "In the IMDB data we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors,we should choose some maximum vocabularly size.Below we use the 5000 mot frequent words.(remebering that stop words have already been removed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialize the \"CountVectorizer\" object , which is scikit-learn's  \n",
    "#Bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None,\n",
    "                            preprocessor = None,stop_words = None,max_features=5000)\n",
    "\n",
    "#fit_transform() does two functions:First , it fits the model # \n",
    "#and learns the vocabularly;second,it transfroms our training data \n",
    "# into feature vecotrs. THe input to fit_transform should be a list of strings. \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews) \n",
    "\n",
    "#Numpy arrays are easy to work with , so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()  \n",
    "\n",
    "test_data_features = vectorizer.fit_transform(clean_test_reviews)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have now 25,000 rows and 5,000 features(one for each vocabularly word)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the Vocabularly \n",
    "# vocab = vectorizer.get_feature_names()  \n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# #Sum up th counts of each vocabulary word  \n",
    "# dist = np.sum(train_data_features,axis=0)  \n",
    "\n",
    "# #For each , pint the vocabularly word and the number of times it appears\n",
    "# #in the training set \n",
    "# for tag, count in zip(vocab,dist):\n",
    "#     print(tag,count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cross Validation\n",
    "X_train = train_data_features\n",
    "y_train = train[\"sentiment\"].values\n",
    "kfold = StratifiedKFold(y=y_train,n_folds=2,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83272000000000002, 0.82976000000000005]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest with cross Validation\n",
    "forest = RandomForestClassifier(n_estimators=100)  \n",
    "scores = [] \n",
    "for k,(tr,te) in enumerate(kfold): \n",
    "    forest.fit(X_train[tr],y_train[tr]) \n",
    "    score = forest.score(X_train[te],y_train[te]) \n",
    "    scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84375999999999995, 0.84423999999999999]\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression With Cross Validation\n",
    "lr = LogisticRegression() \n",
    "scores = [] \n",
    "for k,(tr,te) in enumerate(kfold): \n",
    "    lr.fit(X_train[tr],y_train[tr]) \n",
    "    score = lr.score(X_train[te],y_train[te]) \n",
    "    scores.append(score)\n",
    "print(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79264000000000001, 0.78712000000000004]\n"
     ]
    }
   ],
   "source": [
    "#xgb boost with cross validation\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb =  XGBClassifier() \n",
    "scores = []\n",
    "for k,(tr,te) in enumerate(kfold): \n",
    "    xgb.fit(X_train[tr],y_train[tr]) \n",
    "    score =xgb.score(X_train[te],y_train[te]) \n",
    "    scores.append(score) \n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79503999999999997, 0.79127999999999998]\n"
     ]
    }
   ],
   "source": [
    "#GBM boost with cross validation \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm =GradientBoostingClassifier() \n",
    "scores=[]\n",
    "for k,(tr,te) in enumerate(kfold): \n",
    "    gbm.fit(X_train[tr],y_train[tr]) \n",
    "    score = gbm.score(X_train[te],y_train[te]) \n",
    "    scores.append(score) \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting Data to whole Training Examples now\n",
    "lr.fit(train_data_features,train[\"sentiment\"].values)\n",
    "forest.fit(train_data_features,train[\"sentiment\"].values)\n",
    "xgb.fit(train_data_features,train[\"sentiment\"].values)\n",
    "gbm.fit(train_data_features,train[\"sentiment\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting Test reviews to our Vectorizer\n",
    "test_data_features = vectorizer.fit_transform(clean_test_reviews) \n",
    "\n",
    "#Numpy arrays are easy to work with , so convert the result to an array\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result1 = forest.predict(test_data_features)  \n",
    "\n",
    "# Using the Logistic Regression to make sentiment label predictions\n",
    "result2 = lr.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"rf_one_gram_countVectorizer.csv\", index=False, quoting=3)\n",
    "\n",
    "\n",
    "output2 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result2} )\n",
    "output2.to_csv( \"lr_one_gram_countVectorizer.csv\", index=False, quoting=3 ) \n",
    "\n",
    "#XGB\n",
    "result = xgb.predict(test_data_features)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output1.to_csv( \"xgb_one_gram_countVectorizer.csv\", index=False, quoting=3)\n",
    "\n",
    "#GBM\n",
    "result = gbm.predict(test_data_features)\n",
    "output2 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output2.to_csv( \"gbm_one_gram_countVectorizer.csv\", index=False, quoting=3 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Approach1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest One Gram TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/rf_one_gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression One Gram TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/lr_one_gram.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB One Gram  TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/xgb_onegram_tf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM One Gram TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/gbm_onegram_tf.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Approach 2 :  (TfIDfVectorizer,Onegram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#performing TF-IDF Vectorization on Training Data\n",
    "corpustr = clean_train_reviews #corpusTraining\n",
    "\n",
    "#Making TFidf Vectorizer object\n",
    "vectorizertr = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , \n",
    "                             sublinear_tf=False,max_features =5000)\n",
    "\n",
    "#Fitting the object to Training & Testing Data\n",
    "tfidftr=vectorizertr.fit_transform(corpustr).todense()  \n",
    "corpusts = clean_test_reviews\n",
    "tfidfts=vectorizertr.transform(corpusts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#performing TF-IDF Vectorization on Test Data using TF-IDF object of training\n",
    "#data\n",
    "corpusts = clean_test_reviews\n",
    "tfidfts=vectorizertr.transform(corpusts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors_tr = tfidftr          #Training Data\n",
    "targets_tr = train['sentiment'].values    #Target\n",
    "predictors_ts = tfidfts          #Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(predictors_tr,targets_tr)\n",
    "\n",
    "#Predictions\n",
    "result1=lr.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"lr_onegram_tfidf.csv\", index=False, quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/lr_onegram_tfidf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "forest = RandomForestClassifier() \n",
    "forest=forest.fit(predictors_tr,targets_tr)  \n",
    "\n",
    "#predictions\n",
    "result1=forest.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"rf_onegram_tfidf.csv\", index=False, quoting=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/rf_onegram_tfidf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XGB  \n",
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb =  XGBClassifier()  \n",
    "xgb=xgb.fit(predictors_tr,targets_tr)  \n",
    "\n",
    "#predictions\n",
    "result1=xgb.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"xgb_onegram_tfidf.csv\", index=False, quoting=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GBM \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm =GradientBoostingClassifier() \n",
    "gbm=gbm.fit(predictors_tr,targets_tr)  \n",
    "\n",
    "#predictions\n",
    "result1=gbm.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"gbm_onegram_tfidf.csv\", index=False, quoting=3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3 (TfIDf,Two Gram Model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using HashingVectorizer to control Memory Usage.\n",
    "corpustr = clean_train_reviews\n",
    "estimators = [(\"tfidf\", TfidfVectorizer(stop_words='english',\n",
    "             ngram_range = ( 1 , 1 ),analyzer=\"word\",\n",
    "             max_df = .57 , binary=False ,max_features =6000, \n",
    "             token_pattern=r'\\w+' , sublinear_tf=False) ),\n",
    "             (\"hash\", HashingVectorizer ( stop_words='english',\n",
    "              ngram_range = ( 1 , 2 ),n_features  =6000,\n",
    "            analyzer=\"word\",token_pattern=r'\\w+', binary =False))] \n",
    "\n",
    "tfidftr = FeatureUnion(estimators).fit_transform(corpustr).todense()\n",
    "corpusts = clean_test_reviews\n",
    "tfidfts = FeatureUnion(estimators).transform(corpusts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors_tr = tfidftr          #Training Data\n",
    "targets_tr = train['sentiment'].values    #Target\n",
    "predictors_ts = tfidfts          #Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(predictors_tr,targets_tr)\n",
    "\n",
    "#Predictions\n",
    "result1=lr.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"lr_twogram_tfidf.csv\", index=False, quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/tfidf_lr_8000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modeling using Random Forest   \n",
    "forest = RandomForestClassifier(n_estimators=40) \n",
    "forest=forest.fit(predictors_tr,targets_tr)  \n",
    "\n",
    "#predictions\n",
    "result1=forest.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"rf_twogram_tfidf.csv\", index=False, quoting=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/tfidf_RF_8000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb =  XGBClassifier()  \n",
    "xgb=xgb.fit(predictors_tr,targets_tr)  \n",
    "\n",
    "#predictions\n",
    "result1=xgb.predict(predictors_ts)\n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"xgb_twogram_tfidf.csv\", index=False, quoting=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GBM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm =GradientBoostingClassifier()\n",
    "gbm=gbm.fit(predictors_tr,targets_tr)\n",
    "\n",
    "result1 = gbm.predict(predictors_ts) \n",
    "output1 = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result1} )\n",
    "output1.to_csv( \"gbm_twogram_tfidf.csv\", index=False, quoting=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
